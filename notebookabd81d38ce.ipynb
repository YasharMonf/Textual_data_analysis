{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31259,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://lwfiles.mycourse.app/651ebde6c37afd427e55d85f-public/70e7481517e9de41c30d0c1b1a315182.png\" style=\"width: 600px\"></div>\n","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#00318F; padding: 10px; border-radius: 15px; font-size: 150%; font-family: Verdana; text-align:center; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n   Textual Data Analysis with Python using TextBlob\n</div>","metadata":{}},{"cell_type":"markdown","source":"<div style=\"color:white; background-color:#758FC2; padding: 10px; border-radius: 15px; font-size: 100%; font-family: Verdana; text-align:center; width: 50%; margin: 0 auto; -webkit-text-stroke-width: 1px; -webkit-text-stroke-color: black; text-shadow: 2px 2px 4px rgba(0, 0, 0, 0.7);\">\n   Last Update: Feb 22, 2026\n</br>\n   Notebook created by: Yashar Monfared\n</div>\n","metadata":{}},{"cell_type":"markdown","source":"### Introduction \n\nTextual data analysis has become increasingly important because about 80% of the world's current data is unstructured. This includes emails, blog posts, social media posts, customer reviews, and voice transcripts. With advanced large language models (LLMs), organizations can change this text into useful insights. This can drive automation and improve decision-making. \n\nMost textual data analysis frameworks are work based on language models. Language models are the cornerstone of natural language processing (NLP), offering a range of techniques for understanding and generating human language. Language models are statistical models designed to predict the likelihood of a sequence of words in a given text. They work by estimating the probability distribution of word sequences, which allows them to generate or evaluate sentences based on how frequently certain word combinations appear in a corpus.\n\n\nSome applications of language models:\n\n- Sentiment analysis: Monitoring social media or analyzing customer product reviews.\n- Named Entity Recognition: Automatically indexing news or extracting data from legal contracts.\n- Question Answering: Virtual assistants (Siri, Alexa) and automated customer support bots.\n- Spam Detection: It involves assigning a \"label\" to a piece of text.\n- Grammar and Spell Checking: Look at the Part-of-Speech (POS) context and tags.\n- Speech recognition: Transcribing spoken language into text.\n- Machine translation: Translating text from one language to another.\n- Information retrieval: Ranking documents based on their relevance to a query.\n- Text summarization: Condensing long pieces of text into shorter summaries.\n\n\nThere are different Python libraries for analyzing textual data using traditional language models including **NLTK, TextBlob,** and **spaCy**. In this lesson, we will learn about TextBlob which is a Python library for processing textual data. It provides a simple API for diving into common NLP tasks such as sentiment analysis, part-of-speech tagging, text tokenization, spelling correction, and word frequency counting. With TextBlob, you can perform not only these tasks, but also several other tasks like creating n-grams and lemmatization. We cover some of the basic features of this powerful Python library in this lesson.\n\nTextBlob is a lightweight Python library for NLP that simplifies tasks by acting as a wrapper around NLTK (Natural Language Toolkit) and the pattern library. It operates primarily on rule-based methods and pre-trained lexicons rather than complex, real-time machine learning models or large language models (LLMs), making it efficient for prototyping and smaller applications. TextBlob is ideal for applications requiring high-speed, low-cost, and low resource usage, such as social media monitoring or reviewing the sentiment of simple textual data rather than analyzing long texts which require deep contextual understanding (via deep neural networks or large language models). \n\nTextBlob uses a pre-trained statistical model for most of its language processing tasks powered by traditional language models. Two main traditional language models are Bag-of-Words (BoW) and N-grams (which will be discussed in section 6). \n\nThe BoW model represents text as a collection of words, without considering the order in which they appear. It breaks down a text into individual words, and each word is assigned a count or weight, treating each word as an independent entity.\n\nTextBlob, for most use cases, uses a BoW model combined with its dictionary (the lexicon) to perform various NLP tasks on the textual data. ","metadata":{},"attachments":{}},{"cell_type":"markdown","source":"### 1) Importing TextBlob and Required Packages\n\nWe need to import TextBlob, its wrapper library, NLTK, and the required datasets (dictionaries) and models for TextBlob to perform various NLP tasks on textual data.","metadata":{}},{"cell_type":"code","source":"# Textblob training\n# If you use Kaggle, you don't need to install these libraries but if you use Jupyter Lab\n# On your Local PC, then you need to install these packages:\n# pip install -U textblob\n# python -m textblob.download_corpora\n\n# Installing the required libaries, and datasets for textblob and NLTK\n\nfrom textblob import TextBlob\nimport nltk\nnltk.download('punkt')\nnltk.download('wordnet')\nnltk.download('omw-1.4')\nnltk.download('punkt_tab')\nnltk.download('averaged_perceptron_tagger_eng')\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2026-02-23T01:01:58.623528Z","iopub.execute_input":"2026-02-23T01:01:58.624621Z","iopub.status.idle":"2026-02-23T01:01:59.331945Z","shell.execute_reply.started":"2026-02-23T01:01:58.624582Z","shell.execute_reply":"2026-02-23T01:01:59.33097Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"You may ask what are these nltk.download commands. In simple terms, these are the \"data fuel\" that TextBlob needs to run. TextBlob is the engine, but it doesn't come pre-packaged with the massive dictionaries and statistical rules required to understand English. These nltk.download commands pull specific \"corpora\" (data sets) or \"models\" (trained rules) from the Natural Language Toolkit (NLTK) servers.\n\n- punkt and punkt_tab are Tokenizers (splitting text into words).\n- averaged_perceptron_tagger_eng is the part-of-speech tagger model.\n- wordnet is a massive English lexical database.\n- omw-1.4 (Open Multilingual Wordnet) is an expansion of wordnet.\n\nIf you are working in Kaggle or Google Colab, the virtual machine is \"wiped\" every time you start a new session. Since these files are large, they aren't kept on the base image to save space. You have to download them into the local environment memory each time you run your script. We can use TextBlob() function to create a textblob object in Python:","metadata":{}},{"cell_type":"code","source":"# Let’s create our first TextBlob. We need to feed TextBlob something like plain text.\nsentence1 = TextBlob(\"Python is a high-level programming language.\")\nprint(sentence1)\nprint(type(sentence1))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T23:18:37.8057Z","iopub.execute_input":"2026-02-22T23:18:37.806096Z","iopub.status.idle":"2026-02-22T23:18:37.810902Z","shell.execute_reply.started":"2026-02-22T23:18:37.806067Z","shell.execute_reply":"2026-02-22T23:18:37.80986Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 2) Tokenization\n\nIn TextBlob, tokenization is the process of breaking a large body of text into smaller, meaningful pieces called \"tokens.\" TextBlob makes this incredibly easy by providing two main properties: ***.words*** and ***.sentences***. \n\nWhen you use ***.words***, TextBlob breaks the text down into individual words. Key Feature: Unlike a simple Python *.split()*, TextBlob's word tokenizer is smart. It understands that punctuation (like periods and commas) should be stripped away from the word itself. It also understand \"closed compounds\" or a single word that functions as two distinct grammatical parts (will see an example of this in exercise 1).\n\nWhen you use ***.sentences***, this breaks a large paragraph into a list of individual sentences. It doesn't just look for periods. It uses a pre-trained model (punkt) to distinguish between a period that ends a sentence and a period used in an abbreviation (like \"Mr.\" or \"Dr.\").","metadata":{}},{"cell_type":"code","source":"print(sentence1.words)\nprint(sentence1.sentences)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T23:18:50.956774Z","iopub.execute_input":"2026-02-22T23:18:50.957758Z","iopub.status.idle":"2026-02-22T23:18:50.963001Z","shell.execute_reply.started":"2026-02-22T23:18:50.957723Z","shell.execute_reply":"2026-02-22T23:18:50.961933Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"text = \"\"\"Never forget what you are, for surely the world will not. Make it \nyour strength. Then it can never be your weakness. Armor yourself in it,\nand it will never be used to hurt you.\"\"\"\nsentences_example = TextBlob(text)\nprint(sentences_example.sentences)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T23:20:53.809666Z","iopub.execute_input":"2026-02-22T23:20:53.810647Z","iopub.status.idle":"2026-02-22T23:20:53.816264Z","shell.execute_reply.started":"2026-02-22T23:20:53.81061Z","shell.execute_reply":"2026-02-22T23:20:53.814955Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Exercise 1\n\nTake this famous quote from Martin Luther King Jr.:\n\n\nQuote: \"Darkness cannot drive out darkness; only light can do that. Hate cannot drive out hate; only love can do that.\"\n\n**A)** Break it into tokens using the default ***.words*** property, and count how many tokens are in that quote.\n\n**B)** Break it into words using ***.split()*** method and count how many words are in that quote.\n\n**C)** Compare number of tokens and number of words in the quote, and analzye the result.\n\nHint: You can use len() function to find the number of tokens or words.\n\n\n","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\n\n# 1. Define the text\nquote = \"Darkness cannot drive out darkness; only light can do that. Hate cannot drive out hate; only love can do that.\"\n\n# 2. Create the Blob\nsentence_blob = TextBlob(quote)\n\n# 3. Tokenize into words\ntokens = sentence_blob.words\n\n# 4. Show results\nprint(\"Tokens found:\")\nprint(tokens)\n\nprint(\"\\nTotal token count:\", len(tokens))\n\n# To see the number of words instead of tokens, instead of sentence_blob.words, we use:\nsimple_words = quote.split()\nprint(\"total word count:\", len(simple_words)) # This would give you a different count","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T01:42:37.517331Z","iopub.execute_input":"2026-02-23T01:42:37.518401Z","iopub.status.idle":"2026-02-23T01:42:37.525069Z","shell.execute_reply.started":"2026-02-23T01:42:37.518366Z","shell.execute_reply":"2026-02-23T01:42:37.523951Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As you may notice, the word \"cannot\" is tokenized as two words by TextBlob default tokenizer: \"can\" and \"not\". This is one of the key differences between simple splitting and NLP tokenization. Tokenization, even when using ***.words*** property, is not equal to splitting the text into words!! The reason cannot is split into can and not is that TextBlob’s default tokenizer is designed to follow standard linguistic conventions used in English grammar analysis. In English, \"cannot\" is a single word that functions as two distinct grammatical parts:\n\n- Can: The modal verb (the action/possibility).\n\n- Not: The negation (the denial).\n\nMost advanced NLP tools (like NLTK, which TextBlob uses under the hood) split \"cannot\" so that if you were performing Sentiment Analysis or Part-of-Speech Tagging, the model can clearly see the \"not\" and realize the sentence is being negated. Without splitting it, a simple model might see \"can\" as positive, but it wouldn't \"see\" the negation tucked inside \"cannot\" as easily.","metadata":{}},{"cell_type":"markdown","source":"The ***.words*** and ***.sentences*** properties use the default tokenizers of TextBlob (*textblob.tokenizers.WordTokenizer and textblob.tokenizers.SentenceTokenizer*).\n\nYou can use other tokenizers, such as those provided by NLTK, by passing them into the TextBlob constructor then accessing the tokens property. When you pass a custom tokenizer into the TextBlob constructor, you are overriding the default WordTokenizer. For example, the **TabTokenizer** will only split the text where it finds a *\\t* (tab) character. \n","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nfrom nltk.tokenize import TabTokenizer\ntokenizer = TabTokenizer()\nblob = TextBlob(\"This is\\ta rather tabby\\tblob.\", tokenizer=tokenizer)\nblob.tokens","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T23:27:19.022339Z","iopub.execute_input":"2026-02-22T23:27:19.022629Z","iopub.status.idle":"2026-02-22T23:27:19.028945Z","shell.execute_reply.started":"2026-02-22T23:27:19.022605Z","shell.execute_reply":"2026-02-22T23:27:19.028002Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In this example, TabTokenizer will treat \"This is\" as a single token because there is no tab between them. TextBlob is flexible. While it provides smart defaults, it allows you to 'plug in' specialized tools from the NLTK library to handle non-standard text formats, which you can then access via the ***.tokens*** property","metadata":{}},{"cell_type":"markdown","source":"### 3) Part-of-speech Tagging:\n\nPart-of-Speech (POS) tagging is the process of labeling each word in a sentence with its corresponding grammatical category—such as noun, verb, adjective, or adverb—based on both its definition and its context.\n\nIn NLP, this is a foundational step because the same word can mean very different things depending on its part of speech (e.g., \"The book (noun) is on the table\" vs. \"I need to book (verb) a flight\").","metadata":{}},{"cell_type":"code","source":"# Part-of-speech tags can be accessed through the tags attribute.\nprint(sentence1.tags)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-19T16:32:23.684547Z","iopub.execute_input":"2026-02-19T16:32:23.685309Z","iopub.status.idle":"2026-02-19T16:32:23.877691Z","shell.execute_reply.started":"2026-02-19T16:32:23.685275Z","shell.execute_reply":"2026-02-19T16:32:23.876735Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The output is a list of tuples, where each tuple represents a word and its corresponding POS tag. These tags are assigned using a specific tagging scheme. These are tags in this example: \n\n- NNP stands for \"Proper Noun, Singular\". This indicates that \"Python\" is a specific, named entity.\n- VBZ stands for \"Verb, Present Tense, 3rd Person Singular\". This means \"is\" is a verb in the present tense, used with a third-person singular subject.\n- DT stands for \"Determiner\". This indicates that \"a\" is an article, used to specify a noun.\n- JJ stands for \"Adjective\". This means \"high-level\" is a descriptive word modifying a noun.\n- NN stands for \"Noun, Singular\". This means \"programming\" and \"language\" are both singular noun.\n\nWhen you run blob.tags, the model follows these three steps:\n\n- Tokenization: It breaks your sentence into individual words (tokens).\n\n- Look-up: It checks a simplified internal dictionary for the word.\n\n- Contextual Guessing: If the word can be multiple parts of speech (like \"run\"), it looks at the tags of the words before and after it to decide. If the word is unknown, it looks at the suffix (e.g., words ending in \"-ing\" are likely verbs, \"-ly\" are likely adverbs).","metadata":{}},{"cell_type":"markdown","source":"### 4) Spelling Correction\n\nOne of TextBlob's most user-friendly features is automatic spelling correction. When you call ***.correct()***, TextBlob performs a statistical guess for every word in your string. It uses a combination of two things:\n\n- It compares your words against a large list of correctly spelled words in a known corpus (dataset).\n\n- It calculates how many \"edits\" (insertions, deletions, or swaps of letters) it takes to turn your misspelled word into a real word.","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nsentence3 = TextBlob(\"I havv goood speling!\")\nprint(sentence3.correct())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T23:40:09.436701Z","iopub.execute_input":"2026-02-22T23:40:09.437321Z","iopub.status.idle":"2026-02-22T23:40:09.443438Z","shell.execute_reply.started":"2026-02-22T23:40:09.437286Z","shell.execute_reply":"2026-02-22T23:40:09.442462Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Word objects have a ***Word.spellcheck()*** method that returns a list of (word, confidence) tuples with spelling suggestions.\n","metadata":{}},{"cell_type":"code","source":"from textblob import Word\nword_3 = Word(\"abbility\")\nprint(word_3.spellcheck())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T23:45:31.586999Z","iopub.execute_input":"2026-02-22T23:45:31.58804Z","iopub.status.idle":"2026-02-22T23:45:31.594293Z","shell.execute_reply.started":"2026-02-22T23:45:31.587999Z","shell.execute_reply":"2026-02-22T23:45:31.593136Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here 1.0 means 100% confidence of the model. Model may not be so confident when there are multiple words as alternatives:","metadata":{}},{"cell_type":"code","source":"from textblob import Word\n\nword_4 = Word(\"wormd\")\nprint(word_4.spellcheck())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T23:47:57.145298Z","iopub.execute_input":"2026-02-22T23:47:57.146122Z","iopub.status.idle":"2026-02-22T23:47:57.151729Z","shell.execute_reply.started":"2026-02-22T23:47:57.146082Z","shell.execute_reply":"2026-02-22T23:47:57.150696Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Remember that TextBlob's default corrector is not perfect, it's just a statistical model which process the text \"word-by-word.\" It doesn't always understand the context of the whole sentence. For example, if you type \"I went to the see\" instead of \"sea,\" it might not correct it because \"see\" is already a valid word. \n\nAlso remember that calling .correct() on a massive book (millions of words) can be slow because it has to check every single word against its dictionary.\n\nFinally, if you have medical jargon or very technical slang, TextBlob might try to \"correct\" those into common English words, which might not be what you want!","metadata":{}},{"cell_type":"markdown","source":"#### Exercise 2\n\nUse the ***.correct()*** method to return a polished version of the following sentences:\n\nSentence 1: \"I am realy hapy that the weatherr is sunni today!\"\n</br>\nSentence 2: \"I am realy hapy that the wedder is suny today!\"","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\n\n# A sentence with deliberate typos\nmessy_text1 = \"I am realy hapy that the weatherr is sunni today!\"\nmessy_text2 = \"I am realy hapy that the wedder is suny today!\"\n# Create the TextBlob\nsentence_blob1 = TextBlob(messy_text1)\nsentence_blob2 = TextBlob(messy_text2)\n\n# Apply the correction\nclean_text1 = sentence_blob1.correct()\nclean_text2 = sentence_blob2.correct()\n\nprint(\"Original:\", messy_text1)\nprint(\"Corrected:\",clean_text1)\nprint()\nprint(\"Original 2:\", messy_text2)\nprint(\"Corrected 2:\",clean_text2)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T01:53:56.282552Z","iopub.execute_input":"2026-02-23T01:53:56.283534Z","iopub.status.idle":"2026-02-23T01:53:56.293767Z","shell.execute_reply.started":"2026-02-23T01:53:56.283486Z","shell.execute_reply":"2026-02-23T01:53:56.292738Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The second sentence is not corrected as it was expected! Why is that? This example reveals the limitations of statistical spellcheckers. TextBlob’s ***.correct()*** method doesn't \"read\" the sentence like a human. It looks at each word individually and asks: \"What is the most statistically likely word that looks like this?\". TextBlob sees \"wedder\" and looks for the closest matches.\n\n- Weather: requires changing \"dd\" to \"ath\" (3 changes).\n\n- Redder: only requires changing \"w\" to \"r\" (1 change).\n\nBecause \"redder\" is \"closer\" in terms of typing distance (Edit Distance), the model picks it, even though it makes no sense in the context of the sky!\n\nFurthermore:\n\n- Sunny: requires adding a letter (\"n\").\n\n- Sun: requires removing a letter (\"y\").\n\nIn the specific dataset TextBlob uses, the word \"sun\" is much more common than the adjective \"sunny.\" It assumes you accidentally added a \"y\" to the end of \"sun.\"\n\nThis is one of the major limitations of TextBlob and traditional language models in general: TextBlob is a \"Dictionary\" checker. It looks at words in isolation. Generally, it does not consider the context of words into account!","metadata":{}},{"cell_type":"markdown","source":"### 5) Lemmatization\n\nLemmatization is the process of grouping together the different inflected forms of a word so they can be analyzed as a single item (dictionary form). It’s like looking up the \"root\" of a word in a professional dictionary. Words can be lemmatized by calling the ***.lemmatize()*** method. Here we are using the *Word* object directly. This object uses **WordNet**, a massive lexical database of English, to find the root.","metadata":{}},{"cell_type":"code","source":"from textblob import Word\nword_1 = Word(\"octopi\")\nprint(word_1.lemmatize())\nword_2 = Word(\"went\")\nprint(word_2.lemmatize(\"v\")) \n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T23:35:41.254724Z","iopub.execute_input":"2026-02-22T23:35:41.255125Z","iopub.status.idle":"2026-02-22T23:35:41.261173Z","shell.execute_reply.started":"2026-02-22T23:35:41.255097Z","shell.execute_reply":"2026-02-22T23:35:41.26029Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"TextBlob recognizes that \"octopi\" is the irregular plural of \"octopus.\" A simple stemmer would have just left it as \"octop,\" but the lemmatizer returns the actual base noun. Also, by default, the .lemmatize() method assumes the word is a noun. If you want to lemmatize a verb, you have to tell it by passing the part of speech (\"v\"). Without the \"v\", TextBlob would look for a noun named \"went\" and find nothing, so it would just return \"went.\" By specifying the verb, it correctly identifies \"go\" as the base form.\n\nLemmatization is vital when you want to count word frequencies accurately. Without it, your computer thinks \"run,\" \"ran,\" and \"running\" are three completely different topics. After lemmatization, it knows they are all the same action: \"run.\"","metadata":{}},{"cell_type":"markdown","source":"### 6) N-grams\n\nAn n-gram model is one of the simplest traditional language models. The n-grams functionality in TextBlob is a simple but powerful tool for breaking down text into contiguous sequences of n items (words). This is a standard technique in NLP used to understand the context of words by looking at their neighbors. The idea is that the probability of a word depends on the context of the last few words, with n representing the number of words considered. In symmary, the TextBlob.ngrams() method returns a list of tuples of n successive words. When you call *.ngrams(n=3)* on a TextBlob object, it returns a list of WordList objects. Each WordList contains n consecutive words from your text.\n\n\n- Unigram (n=1): Only the current word is considered (no context).\n- Bigram (n=2): The model considers one previous word.\n- Trigram (n=3): The model considers two previous words, and so on.\n\n\n\n","metadata":{},"attachments":{}},{"cell_type":"code","source":"blob = TextBlob(\"Now is better than never.\")\nprint(blob.ngrams(n=3))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T22:33:23.017755Z","iopub.execute_input":"2026-02-22T22:33:23.018111Z","iopub.status.idle":"2026-02-22T22:33:23.023697Z","shell.execute_reply.started":"2026-02-22T22:33:23.018083Z","shell.execute_reply":"2026-02-22T22:33:23.022509Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In NLP, n-grams are essential for tasks where single words don't provide enough meaning:\n\n- Sentiment Context: \"Not happy\" is a bigram. If you only look at unigrams (\"not\" and \"happy\"), a model might think the text is positive because of the word \"happy.\" The bigram captures the negation.\n\n- Auto-complete/Next-word Prediction: By analyzing which words frequently follow others (e.g., \"New York\" vs. \"New Table\"), models can predict the next word in a sequence.\n\n- Entity Recognition: Identifying phrases like \"Social Media\" or \"Artificial Intelligence\" as single concepts rather than separate words.\n\n\nIn order to count the frequency of n-grams, we can use Counter function from collection library.\n","metadata":{}},{"cell_type":"code","source":"# counting the top 3 bigrams in the text\n\nfrom textblob import TextBlob\nfrom collections import Counter\n\ntext= \"\"\"\nSocial science is the study of people: as individuals, communities and societies; their behaviours and interactions with each other and with their built, technological and natural environments. Social science seeks to understand the evolving human systems across our increasingly complex world and how our planet can be more sustainably managed. It’s vital to our shared future.\n\nSocial science includes many different areas of study, such as how people organise and govern themselves, and broker power and international relations; how wealth is generated, economies develop, and economic futures are modelled; how business works and what a sustainable future means; the ways in which populations are changing, and issues of unemployment, deprivation and inequality; and how these social, cultural and economic dynamics vary in different places, with different outcomes.\n\"\"\"\n\n# 1. Initialize TextBlob and create bigrams\nblob = TextBlob(text)\nbigrams = blob.ngrams(n=2)\n\n# 2. Create an empty list to store the \"string\" versions\n# The Counter tool is designed to count individual items in a list.\nbigram_list = []\n\n# 3. Loop through and add them to our list\nfor word in bigrams:\n    # word is something like ['social', 'science']\n    # We turn it into \"social science\" and add it to the list\n    bigram_as_string = str(word[0] + \" \" + word[1])\n    bigram_list.append(bigram_as_string)\n\n# 3. Use Counter to find the most frequent pairs\nbigram_counts = Counter(bigram_list)\n\n# 4. Extract the top 3\ntop_3_bigrams = bigram_counts.most_common(3)\n\nprint(\"Top 3 Bigrams:\")\nfor bigram, count in top_3_bigrams:\n    print(bigram,\":\", count)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T22:36:19.850704Z","iopub.execute_input":"2026-02-22T22:36:19.851661Z","iopub.status.idle":"2026-02-22T22:36:19.862117Z","shell.execute_reply.started":"2026-02-22T22:36:19.851627Z","shell.execute_reply":"2026-02-22T22:36:19.861005Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### Exercise 3)\n\nFind the most frequency unigram in the following body of text:\n\n\"Never forget what you are, for surely the world will not. Make it your strength. Then it can never be your weakness. Armor yourself in it, and it will never be used to hurt you.\"","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nfrom collections import Counter\n\ntext = \"\"\"Never forget what you are, for surely the world will not. Make it \nyour strength. Then it can never be your weakness. Armor yourself in it,\nand it will never be used to hurt you.\"\"\"\n\n# 1. Initialize TextBlob and create unigrams\nblob = TextBlob(text)\nunigrams = blob.ngrams(n=1)\n\n# 2. Create an empty list to store the \"string\" versions\n# The Counter tool is designed to count individual items in a list.\nunigram_list = []\n\n# 3. Loop through and add them to our list\nfor word in unigrams:\n    # word is something like ['social', 'science']\n    # We turn it into \"social science\" and add it to the list\n    unigram_as_string = str(word[0])\n    unigram_list.append(unigram_as_string)\n\n# 3. Use Counter to find the most frequent pairs\nunigram_counts = Counter(unigram_list)\n\n# 4. Extract the top unigram\ntop_unigram = unigram_counts.most_common(1)\n\nprint(\"Top unigram:\",top_unigram)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T02:11:37.829911Z","iopub.execute_input":"2026-02-23T02:11:37.830679Z","iopub.status.idle":"2026-02-23T02:11:37.841125Z","shell.execute_reply.started":"2026-02-23T02:11:37.830639Z","shell.execute_reply":"2026-02-23T02:11:37.839854Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"In most real-world applications, we need to exclude words like \"the\" and \"it\" from our n-grams analysis and count, as they are usually meaningful words for various NLP tasks. We also need to lower case n-grams to avoid counting \"communication\" and \"Communication\" as two different words! To filter out \"stop words\" from your n-grams, you combine Python's list comprehension with a list of common words to ignore (like \"the\", \"is\", \"at\"). This is essential for finding the actual meaning in a text, rather than just seeing \"of the\" or \"in a\" repeatedly. Here is the most efficient way to do it using NLTK’s stop word list:","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nfrom collections import Counter\nimport nltk\nfrom nltk.corpus import stopwords\n\n# Ensure the stop words are downloaded\nnltk.download('stopwords')\nstop_words = set(stopwords.words('english'))\n\ntext = \"\"\"Never forget what you are, for surely the world will not. Make it \nyour strength. Then it can never be your weakness. Armor yourself in it,\nand it will never be used to hurt you.\"\"\"\n\n# 1. Initialize TextBlob and create unigrams (n=1)\nblob = TextBlob(text)\nunigrams = blob.ngrams(n=1)\n\n# 2. Create an empty list for the filtered words\nunigram_list = []\n\n# 3. Loop through and add only if it's NOT a stop word\nfor word in unigrams:\n    unigram_as_string = str(word[0]).lower() # Convert to lower to match stop_words list\n    \n    # NEW: Check if the word is in the stop words list\n    if unigram_as_string not in stop_words:\n        unigram_list.append(unigram_as_string)\n\n# 4. Use Counter to find the most frequent meaningful word\nunigram_counts = Counter(unigram_list)\n\n# 5. Extract the top unigram\ntop_unigram = unigram_counts.most_common(1)\n\nprint(\"Top unigram (filtered):\", top_unigram)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-22T23:10:21.483247Z","iopub.execute_input":"2026-02-22T23:10:21.483643Z","iopub.status.idle":"2026-02-22T23:10:21.493906Z","shell.execute_reply.started":"2026-02-22T23:10:21.483612Z","shell.execute_reply":"2026-02-22T23:10:21.492703Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 7) Sentiment analysis\n\nThe lexicon is a pre-defined dictionary. It contains 3,546 words and phrases, each annotated with its sentiment and other language characteristics. The lexicon is the foundation of TextBlob's default, rule-based sentiment analyzer. The Lexcicon has specifically a databse named en-sentiment.xml, which maps thousands of English words to their respective sentiment polarity (positive/negative) and subjectivity (objective/subjective) scores.  These are steps that TextBlob takes before showing you the sentiment scores for a body of text:\n\n- Tokenization: TextBlob splits a sentence into individual words.\n- Lookup: It searches for each word in the en-sentiment.xml lexicon.\n- Aggregation: It computes the overall sentiment by averaging the polarity and subjectivity scores of all recognized words.\n\nOne type of sentiment analysis is to calculate the overall positivity or negativity of a text corpus. There are a variety of algorithms and scales. TextBlob’s default ***.sentiment*** function rates an input text as negative or positive on a scale of -1 to 1 (Lexicon Approach). The sentiment function returns two numbers in the form of a named tuple:\n\n**Sentiment(polarity, subjectivity)**\n\nThe polarity score is a float within the range [-1.0, 1.0]. Subjectivity is a float within the range [0.0, 1.0] where 0.0 is very objective (ie: does not express strong sentiment) and 1.0 is very subjective.\n","metadata":{}},{"cell_type":"code","source":"testimonial = TextBlob(\"Textblob is amazingly simple to use. What a fun library!\")\n\nprint(testimonial.sentiment)\nprint(testimonial.sentiment.polarity)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T11:52:46.7308Z","iopub.execute_input":"2026-02-18T11:52:46.731157Z","iopub.status.idle":"2026-02-18T11:52:46.782243Z","shell.execute_reply.started":"2026-02-18T11:52:46.731128Z","shell.execute_reply":"2026-02-18T11:52:46.781187Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"When you call ***TextBlob(text).sentiment***, the library:\n\n- Tokenizes the text into words.\n- Identifies words present in its sentiment lexicon.\n- Calculates an average polarity and subjectivity score for the entire sentence based on - individual word scores.\n- Adjusts for negations or modifiers","metadata":{}},{"cell_type":"code","source":"sentence2 = TextBlob(\"This product is not great, it is very slow!\")\nprint(sentence2.sentiment)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T11:55:05.303506Z","iopub.execute_input":"2026-02-18T11:55:05.303858Z","iopub.status.idle":"2026-02-18T11:55:05.31016Z","shell.execute_reply.started":"2026-02-18T11:55:05.30383Z","shell.execute_reply":"2026-02-18T11:55:05.309313Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here, polarity represents the sentiment of the text, ranging from -1 to 1:\n-1: Very negative sentiment\n0: Neutral sentiment\n1: Very positive sentiment\nThe closer the polarity is to 1, the more positive the sentiment is. The closer the polarity is to -1, the more negative the sentiment is. If it's close to 0, it suggests neutral sentiment. Subjectivity measures how subjective or objective the text is, with a range from 0 to 1:\n\n\t0: Very objective (factual)\n\t1: Very subjective (opinion-based)\n\nHigher subjectivity values indicate that the text is more opinion-based or emotional, while lower values indicate that the text is more factual or neutral.\n","metadata":{}},{"cell_type":"markdown","source":"#### Exercise 4 \n\nUsing TextBlob, determine the whether the sentiment of the following four Amazon reviews is positive or negative (determine the sentiment for each sentence separately). You should determine whether it is positive or negative by printing out \"positive\" or \"negative\" for their sentiments.\n\n- Review 1:  The computer speed is great, I love it!\n- Review 2:  The computer is extremely slow and unreliable.\n- Review 3:  It worth the money but it's just average.\n- Review 4:  It is so fast that I can see bar load loading!\n\nDid you see any limitations when using TextBlob for sentiment analysis?\n","metadata":{}},{"cell_type":"code","source":"reviews = [\n    \"The computer speed is great, I love it!\",\n    \"The computer is extremely slow and unreliable.\",\n    \"It worth the money but it's just average.\",\n    \"It is so fast that I can see bar load loading!\"\n]\n\nfor i, review_text in enumerate(reviews, 1):\n    blob = TextBlob(review_text)\n    \n    print(f\"Review {i}:\")\n    for sentence in blob.sentences:\n        # Determine sentiment based on polarity\n        if sentence.sentiment.polarity > 0:\n            sentiment_label = \"positive\"\n        else:\n            sentiment_label = \"negative\"\n            \n        print(f\"  Sentence: '{sentence}' -> {sentiment_label}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-18T12:02:11.943437Z","iopub.execute_input":"2026-02-18T12:02:11.943761Z","iopub.status.idle":"2026-02-18T12:02:11.952319Z","shell.execute_reply.started":"2026-02-18T12:02:11.943733Z","shell.execute_reply":"2026-02-18T12:02:11.951331Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Note: TextBlob is built on top of the pattern library, allowing for efficient sentiment lookup, but it is less capable of understanding context-dependent, nuanced, or sarcastic text compared to modern large language models.\n\nWhy it lacks deep \"Conceptual Understanding\":\n\n- Lexicon-Based Approach: By default, TextBlob's sentiment analyzer is lexicon-based, meaning it calculates sentiment by averaging scores of predefined words and phrases, not by truly understanding the concept behind them.\n- No Vectorization/Semantic Depth: TextBlob does not provide advanced features like word vectors or deep neural networks that allow for semantic understanding of relationships between words.\n- Limited Context: While n-grams are better than unigrams (single words), they only capture local, immediate context (the 2 or 3 words next to each other), not the broader conceptual meaning of a sentence or paragraph\n\nThe textblob.sentiments module contains two sentiment analysis implementations, **PatternAnalyzer** (based on the pattern library) and **NaiveBayesAnalyzer** (an NLTK classifier trained on a movie reviews corpus).\n\nThe default implementation is PatternAnalyzer, but you can override the analyzer by passing another implementation into a TextBlob’s constructor.\n\nFor instance, the NaiveBayesAnalyzer returns its result as a namedtuple of the form: Sentiment(classification, p_pos, p_neg).\n","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nfrom textblob.sentiments import NaiveBayesAnalyzer\nreview_example = TextBlob(\"I love this library\", analyzer=NaiveBayesAnalyzer())\nprint(review_example.sentiment)\nprint(review_example.sentiment.classification)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T00:53:18.898549Z","iopub.execute_input":"2026-02-23T00:53:18.899992Z","iopub.status.idle":"2026-02-23T00:53:22.585611Z","shell.execute_reply.started":"2026-02-23T00:53:18.899939Z","shell.execute_reply":"2026-02-23T00:53:22.584504Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As mentioned before, by default, TextBlob uses a \"PatternAnalyzer\" (based on a simple word-dictionary), but here you are swapping it out for a more advanced machine learning model. Instead of just looking for \"good\" or \"bad\" words, it calculates the probability that a sentence belongs to the \"positive\" class versus the \"negative\" class based on patterns it learned during training. It is often more \"opinionated\" than the default analyzer. It doesn't just give you a polarity score between -1 and 1; it gives you a specific classification.\n\n- classification: The model's final \"vote\" (either 'pos' for positive or 'neg' for negative).\n\n- p_pos: The calculated probability that the text is positive.\n\n- p_neg: The calculated probability that the text is negative.\n\nIn this example \"I love this library\", the model will see the word \"love\" (which appears frequently in positive movie reviews) and will likely assign a very high p_pos (positive probability) and classify it as 'pos'","metadata":{}},{"cell_type":"markdown","source":"### 8) TextBlob and Python Strings\n\nTextBlobs objects are Like Python strings, meaning they support slicing and indexing. TextBlob was designed to feel like a natural extension of Python’s built-in str type, so you don't have to learn new syntax to grab specific parts of your text. Just like a string, you can use square brackets [] to grab a character at a specific position. Remember that Python starts counting at 0. You can also use the [start:stop] syntax to \"slice\" out a portion of the text. You can finally use string methods like ***.upper()***. ","metadata":{}},{"cell_type":"code","source":"sentences_example2 = TextBlob(\n\n    \"Beautiful is better than ugly. \"\n\n    \"Explicit is better than implicit. \"\n\n    \"Simple is better than complex.\"\n\n)\nprint(sentences_example2[0])\nprint(sentences_example2[0:19])\nprint(sentences_example2.upper())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T00:57:20.744186Z","iopub.execute_input":"2026-02-23T00:57:20.744511Z","iopub.status.idle":"2026-02-23T00:57:20.751479Z","shell.execute_reply.started":"2026-02-23T00:57:20.744486Z","shell.execute_reply":"2026-02-23T00:57:20.750132Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Remember when you slice a TextBlob or use a method like .upper(), the result isn't just a plain string—it's a new TextBlob object. This means the \"child\" slice inherits all the \"DNA\" of the parent, allowing you to run NLP tasks (like sentiment or tagging) on that specific fragment immediately.","metadata":{}},{"cell_type":"markdown","source":"#### Exercise 5\n\nIn NLP, we often want to isolate specific parts of a document to see how the tone shifts. In the following text provided, slice the string to skip the \"hateful\" part and only analyze the the \"loving\" part.\n\nUsing TextBlob, calculate the sentiment polarity of the second sentence in this text (I love coding):\n\nI hate chores. I love coding!\n\n","metadata":{}},{"cell_type":"code","source":"from textblob import TextBlob\nsentence_5= TextBlob(\"I hate chores. I love coding!\")\n\n# Slice the second sentence and check its mood immediately\nprint(sentence_5[15:].sentiment.polarity)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T00:59:58.441599Z","iopub.execute_input":"2026-02-23T00:59:58.441967Z","iopub.status.idle":"2026-02-23T00:59:58.448554Z","shell.execute_reply.started":"2026-02-23T00:59:58.44194Z","shell.execute_reply":"2026-02-23T00:59:58.447026Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### 9) How to Import Textual Data from Web\n\nIn real world applications, we usually need to import textual data from a text file, usually a raw text file. Let’s try to first create a text file using a text editor (nano, or even Jupyter lab text editor). Open a blank text file and type in a sentence like: \n\n*“This is a simple sentence that I typed for illustration purposes. I do not intend to use it for anything else.”* \n\nOnce you finish typing, save the text file with a name mytext.txt and proceed to your notebook again. This is the code we can use to import this text file and use textblob tagging on the data: \n","metadata":{}},{"cell_type":"code","source":"import requests\nfrom textblob import TextBlob\n\n# 1. Use the \"RAW\" version of the GitHub URL\nurl = \"https://raw.githubusercontent.com/YasharMonf/Textual_data_analysis/main/mytext.txt\"\n\n# 2. Fetch the document data from the web\nresponse = requests.get(url)\n\n# 3. Get the text content from the response\ndocument = response.text\n\nprint(\"Data type:\", type(document))\n\n# 4. Process the document text with TextBlob\ndocument_blob = TextBlob(document)\nprint(document_blob.sentiment.subjectivity)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2026-02-23T01:04:15.670725Z","iopub.execute_input":"2026-02-23T01:04:15.671135Z","iopub.status.idle":"2026-02-23T01:04:15.781883Z","shell.execute_reply.started":"2026-02-23T01:04:15.671105Z","shell.execute_reply":"2026-02-23T01:04:15.780693Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To wrap up this session, it’s important to understand where TextBlob shines and where it hits a wall, especially as we prepare to transition into the world of Large Language Models (LLMs). \n\nAdvantages of TextBlob:\n\n- Extremely Beginner-Friendly: If you know how to use a Python string, you practically already know how to use TextBlob.\n\n- Built-in properties and methods: It bundles complex tasks (tagging, sentiment, spelling correction) into simple properties. \n\n- Efficient and Fast: Because it uses rule-based logic and lightweight statistical models, it can process thousands of short sentences in seconds on a standard laptop.\n\n- Logic Transparency: Unlike \"black box\" AI models, TextBlob’s default sentiment is based on a dictionary. You can actually look up why a word like \"excellent\" gave a score of 1.0.\n\nLimitations of TextBlob:\n\n- Context Blindness: TextBlob often struggles with sarcasm or double negatives. For example, it might struggle to understand that \"not bad\" is actually \"good.\"\n\n- Limited Deep Understanding: It treats language more like a bag of words than a coherent thought. It doesn't understand the relationship between ideas across long paragraphs.\n\n- Dependency on NLTK Corpora: As you saw today, you have to manually download data packages (punkt, wordnet) to make it work, which can be a hurdle for deployment.\n\n- Language Limitations: The core features are heavily optimized for English and may perform poorly on other languages.","metadata":{}},{"cell_type":"markdown","source":"## References and further information\n\nTextBlob GitHub page: https://github.com/sloria/textblob\n\n\nTextBlob official documentation: https://textblob.readthedocs.io/en/dev/","metadata":{}}]}